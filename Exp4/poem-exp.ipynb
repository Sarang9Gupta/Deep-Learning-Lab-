{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11147207,"sourceType":"datasetVersion","datasetId":6954157}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport csv\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:15:43.544585Z","iopub.execute_input":"2025-03-24T10:15:43.544932Z","iopub.status.idle":"2025-03-24T10:15:43.548825Z","shell.execute_reply.started":"2025-03-24T10:15:43.544868Z","shell.execute_reply":"2025-03-24T10:15:43.548068Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"# Load the Dataset\ntext = \"\"\nwith open(\"/kaggle/input/poemds/poems-100.csv\", \"r\") as file:\n    reader = csv.reader(file)\n    for row in reader:\n        text += \" \".join(row) + \" \"                          \n# Combine All Lines into a Single Text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:15:45.925668Z","iopub.execute_input":"2025-03-24T10:15:45.925971Z","iopub.status.idle":"2025-03-24T10:15:45.933821Z","shell.execute_reply.started":"2025-03-24T10:15:45.925948Z","shell.execute_reply":"2025-03-24T10:15:45.933167Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"# Tokenize the Text into Words\ntokens = text.split()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:15:48.354172Z","iopub.execute_input":"2025-03-24T10:15:48.354446Z","iopub.status.idle":"2025-03-24T10:15:48.359985Z","shell.execute_reply.started":"2025-03-24T10:15:48.354423Z","shell.execute_reply":"2025-03-24T10:15:48.359157Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"# Create a Dictionary to Map Words to Indices\nword_to_idx = {}\nidx_to_word = {}\nvocab_size = 0\n\nfor word in tokens:\n    if word not in word_to_idx:\n        word_to_idx[word] = vocab_size\n        idx_to_word[vocab_size] = word\n        vocab_size += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:15:51.086296Z","iopub.execute_input":"2025-03-24T10:15:51.086585Z","iopub.status.idle":"2025-03-24T10:15:51.097621Z","shell.execute_reply.started":"2025-03-24T10:15:51.086562Z","shell.execute_reply":"2025-03-24T10:15:51.096858Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"print(f\"Vocabulary Size: {vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:15:55.384534Z","iopub.execute_input":"2025-03-24T10:15:55.384864Z","iopub.status.idle":"2025-03-24T10:15:55.389330Z","shell.execute_reply.started":"2025-03-24T10:15:55.384834Z","shell.execute_reply":"2025-03-24T10:15:55.388354Z"}},"outputs":[{"name":"stdout","text":"Vocabulary Size: 7460\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"# Convert Tokens to Indices\ntoken_indices = [word_to_idx[word] for word in tokens]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:15:58.451048Z","iopub.execute_input":"2025-03-24T10:15:58.451341Z","iopub.status.idle":"2025-03-24T10:15:58.457623Z","shell.execute_reply.started":"2025-03-24T10:15:58.451319Z","shell.execute_reply":"2025-03-24T10:15:58.456817Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"# Create Sequences and Targets\nseq_length = 10\nsequences = []\ntargets = []\n\nfor i in range(len(token_indices) - seq_length):\n    seq = token_indices[i:i + seq_length]\n    target = token_indices[i + seq_length]\n    sequences.append(seq)\n    targets.append(target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:01.668432Z","iopub.execute_input":"2025-03-24T10:16:01.668716Z","iopub.status.idle":"2025-03-24T10:16:01.690350Z","shell.execute_reply.started":"2025-03-24T10:16:01.668693Z","shell.execute_reply":"2025-03-24T10:16:01.689563Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"# Convert to PyTorch Tensors\nsequences = torch.tensor(sequences, dtype = torch.long)\ntargets = torch.tensor(targets, dtype = torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:05.178756Z","iopub.execute_input":"2025-03-24T10:16:05.179123Z","iopub.status.idle":"2025-03-24T10:16:05.204122Z","shell.execute_reply.started":"2025-03-24T10:16:05.179092Z","shell.execute_reply":"2025-03-24T10:16:05.203237Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"# Define One-Hot Encoding for RNN Model\nclass OneHotRNN(nn.Module):\n    def __init__(self, vocab_size, hidden_dim, output_dim):\n        super(OneHotRNN, self).__init__()\n        self.rnn = nn.RNN(vocab_size, hidden_dim, batch_first = True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        output, _ = self.rnn(x)\n        out = self.fc(output[:, -1, :])\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:08.149962Z","iopub.execute_input":"2025-03-24T10:16:08.150251Z","iopub.status.idle":"2025-03-24T10:16:08.154966Z","shell.execute_reply.started":"2025-03-24T10:16:08.150229Z","shell.execute_reply":"2025-03-24T10:16:08.154136Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"# Define LSTM Model with Embedding Layer\nclass PoemLSTM(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n        super(PoemLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first = True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        output, _ = self.lstm(embedded)\n        out = self.fc(output[:, -1, :])\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:11.807180Z","iopub.execute_input":"2025-03-24T10:16:11.807465Z","iopub.status.idle":"2025-03-24T10:16:11.812352Z","shell.execute_reply.started":"2025-03-24T10:16:11.807444Z","shell.execute_reply":"2025-03-24T10:16:11.811378Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"# Hyperparameters\nembed_dim = 100\nhidden_dim = 128\noutput_dim = vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:15.596311Z","iopub.execute_input":"2025-03-24T10:16:15.596593Z","iopub.status.idle":"2025-03-24T10:16:15.600162Z","shell.execute_reply.started":"2025-03-24T10:16:15.596571Z","shell.execute_reply":"2025-03-24T10:16:15.599243Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"# Initialize Models\nonehot_model = OneHotRNN(vocab_size, hidden_dim, output_dim)\nembedding_model = PoemLSTM(vocab_size, embed_dim, hidden_dim, output_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:18.271281Z","iopub.execute_input":"2025-03-24T10:16:18.271566Z","iopub.status.idle":"2025-03-24T10:16:18.300155Z","shell.execute_reply.started":"2025-03-24T10:16:18.271545Z","shell.execute_reply":"2025-03-24T10:16:18.299537Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nonehot_optimizer = optim.Adam(onehot_model.parameters(), lr = 0.001)\nembedding_optimizer = optim.Adam(embedding_model.parameters(), lr = 0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:21.401785Z","iopub.execute_input":"2025-03-24T10:16:21.402123Z","iopub.status.idle":"2025-03-24T10:16:21.406627Z","shell.execute_reply.started":"2025-03-24T10:16:21.402097Z","shell.execute_reply":"2025-03-24T10:16:21.405896Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"# Loss Tracking\nonehot_losses, embedding_losses = [], []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:23.831524Z","iopub.execute_input":"2025-03-24T10:16:23.831817Z","iopub.status.idle":"2025-03-24T10:16:23.835433Z","shell.execute_reply.started":"2025-03-24T10:16:23.831795Z","shell.execute_reply":"2025-03-24T10:16:23.834557Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"# Training Function with Tracking\ndef train_model(model, optimizer, name):\n    start_time = time.time()\n    for epoch in range(100):\n        total_loss = 0\n        for i in range(0, len(sequences), 32):\n            batch_seq = sequences[i:i + 32]\n            batch_target = targets[i:i + 32]\n\n            # One-Hot Encoding for OneHotRNN\n            if name == \"OneHotRNN\":\n                batch_seq = F.one_hot(batch_seq, num_classes = vocab_size).float()\n\n            # Forward Pass\n            outputs = model(batch_seq)\n            loss = criterion(outputs, batch_target)\n\n            # Backward Pass and Optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / (len(sequences) // 32)\n        if name == \"OneHotRNN\":\n            onehot_losses.append(avg_loss)\n        else:\n            embedding_losses.append(avg_loss)\n\n        print(f\"{name} Epoch [{epoch+1}/100], Avg Loss: {avg_loss:.4f}\")\n    print(f\"{name} Training Time: {time.time() - start_time:.2f}s\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:28.593145Z","iopub.execute_input":"2025-03-24T10:16:28.593427Z","iopub.status.idle":"2025-03-24T10:16:28.599066Z","shell.execute_reply.started":"2025-03-24T10:16:28.593406Z","shell.execute_reply":"2025-03-24T10:16:28.598155Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"# Poem Generation Function\ndef generate_poem(model, seed_text, num_words = 50, model_type = \"EmbeddingLSTM\"):\n    model.eval()\n    words = seed_text.split()\n    with torch.no_grad():\n        for _ in range(num_words):\n            seq = [word_to_idx.get(word, 0) for word in words[-seq_length:]]\n            seq = torch.tensor(seq, dtype = torch.long).unsqueeze(0)\n\n            if model_type == \"OneHotRNN\":\n                seq = F.one_hot(seq, num_classes = vocab_size).float()\n\n            output = model(seq)\n            probabilities = F.softmax(output, dim = 1)\n            predicted_idx = torch.multinomial(probabilities, 1).item()\n\n            words.append(idx_to_word[predicted_idx])\n\n    return \" \".join(words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:32.878114Z","iopub.execute_input":"2025-03-24T10:16:32.878398Z","iopub.status.idle":"2025-03-24T10:16:32.883713Z","shell.execute_reply.started":"2025-03-24T10:16:32.878374Z","shell.execute_reply":"2025-03-24T10:16:32.882945Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"# Train Models\ntrain_model(onehot_model, onehot_optimizer, \"OneHotRNN\")\ntrain_model(embedding_model, embedding_optimizer, \"EmbeddingLSTM\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T10:16:36.839279Z","iopub.execute_input":"2025-03-24T10:16:36.839564Z","iopub.status.idle":"2025-03-24T11:00:06.548755Z","shell.execute_reply.started":"2025-03-24T10:16:36.839543Z","shell.execute_reply":"2025-03-24T11:00:06.547993Z"}},"outputs":[{"name":"stdout","text":"OneHotRNN Epoch [1/100], Avg Loss: 7.5849\nOneHotRNN Epoch [2/100], Avg Loss: 6.7192\nOneHotRNN Epoch [3/100], Avg Loss: 6.3652\nOneHotRNN Epoch [4/100], Avg Loss: 6.1406\nOneHotRNN Epoch [5/100], Avg Loss: 5.9428\nOneHotRNN Epoch [6/100], Avg Loss: 5.8024\nOneHotRNN Epoch [7/100], Avg Loss: 5.5409\nOneHotRNN Epoch [8/100], Avg Loss: 5.3024\nOneHotRNN Epoch [9/100], Avg Loss: 5.1020\nOneHotRNN Epoch [10/100], Avg Loss: 4.9088\nOneHotRNN Epoch [11/100], Avg Loss: 4.6854\nOneHotRNN Epoch [12/100], Avg Loss: 4.4943\nOneHotRNN Epoch [13/100], Avg Loss: 4.2669\nOneHotRNN Epoch [14/100], Avg Loss: 4.0271\nOneHotRNN Epoch [15/100], Avg Loss: 3.8095\nOneHotRNN Epoch [16/100], Avg Loss: 3.6117\nOneHotRNN Epoch [17/100], Avg Loss: 3.4663\nOneHotRNN Epoch [18/100], Avg Loss: 3.2921\nOneHotRNN Epoch [19/100], Avg Loss: 3.0858\nOneHotRNN Epoch [20/100], Avg Loss: 2.8735\nOneHotRNN Epoch [21/100], Avg Loss: 2.7058\nOneHotRNN Epoch [22/100], Avg Loss: 2.5675\nOneHotRNN Epoch [23/100], Avg Loss: 2.4161\nOneHotRNN Epoch [24/100], Avg Loss: 2.2587\nOneHotRNN Epoch [25/100], Avg Loss: 2.1252\nOneHotRNN Epoch [26/100], Avg Loss: 2.0070\nOneHotRNN Epoch [27/100], Avg Loss: 1.8420\nOneHotRNN Epoch [28/100], Avg Loss: 1.6550\nOneHotRNN Epoch [29/100], Avg Loss: 1.4836\nOneHotRNN Epoch [30/100], Avg Loss: 1.3472\nOneHotRNN Epoch [31/100], Avg Loss: 1.2230\nOneHotRNN Epoch [32/100], Avg Loss: 1.1325\nOneHotRNN Epoch [33/100], Avg Loss: 1.0665\nOneHotRNN Epoch [34/100], Avg Loss: 0.9989\nOneHotRNN Epoch [35/100], Avg Loss: 0.9219\nOneHotRNN Epoch [36/100], Avg Loss: 0.8416\nOneHotRNN Epoch [37/100], Avg Loss: 0.7535\nOneHotRNN Epoch [38/100], Avg Loss: 0.6756\nOneHotRNN Epoch [39/100], Avg Loss: 0.5990\nOneHotRNN Epoch [40/100], Avg Loss: 0.5266\nOneHotRNN Epoch [41/100], Avg Loss: 0.4675\nOneHotRNN Epoch [42/100], Avg Loss: 0.4232\nOneHotRNN Epoch [43/100], Avg Loss: 0.3749\nOneHotRNN Epoch [44/100], Avg Loss: 0.3316\nOneHotRNN Epoch [45/100], Avg Loss: 0.2974\nOneHotRNN Epoch [46/100], Avg Loss: 0.2663\nOneHotRNN Epoch [47/100], Avg Loss: 0.2446\nOneHotRNN Epoch [48/100], Avg Loss: 0.2173\nOneHotRNN Epoch [49/100], Avg Loss: 0.1985\nOneHotRNN Epoch [50/100], Avg Loss: 0.1848\nOneHotRNN Epoch [51/100], Avg Loss: 0.1667\nOneHotRNN Epoch [52/100], Avg Loss: 0.1567\nOneHotRNN Epoch [53/100], Avg Loss: 0.1394\nOneHotRNN Epoch [54/100], Avg Loss: 0.1247\nOneHotRNN Epoch [55/100], Avg Loss: 0.1099\nOneHotRNN Epoch [56/100], Avg Loss: 0.0999\nOneHotRNN Epoch [57/100], Avg Loss: 0.0959\nOneHotRNN Epoch [58/100], Avg Loss: 0.0969\nOneHotRNN Epoch [59/100], Avg Loss: 0.0895\nOneHotRNN Epoch [60/100], Avg Loss: 0.0811\nOneHotRNN Epoch [61/100], Avg Loss: 0.0723\nOneHotRNN Epoch [62/100], Avg Loss: 0.0772\nOneHotRNN Epoch [63/100], Avg Loss: 0.0696\nOneHotRNN Epoch [64/100], Avg Loss: 0.0634\nOneHotRNN Epoch [65/100], Avg Loss: 0.0608\nOneHotRNN Epoch [66/100], Avg Loss: 0.0559\nOneHotRNN Epoch [67/100], Avg Loss: 0.0514\nOneHotRNN Epoch [68/100], Avg Loss: 0.0501\nOneHotRNN Epoch [69/100], Avg Loss: 0.0462\nOneHotRNN Epoch [70/100], Avg Loss: 0.0483\nOneHotRNN Epoch [71/100], Avg Loss: 0.0458\nOneHotRNN Epoch [72/100], Avg Loss: 0.0517\nOneHotRNN Epoch [73/100], Avg Loss: 0.0430\nOneHotRNN Epoch [74/100], Avg Loss: 0.0358\nOneHotRNN Epoch [75/100], Avg Loss: 0.0371\nOneHotRNN Epoch [76/100], Avg Loss: 0.0410\nOneHotRNN Epoch [77/100], Avg Loss: 0.0324\nOneHotRNN Epoch [78/100], Avg Loss: 0.0354\nOneHotRNN Epoch [79/100], Avg Loss: 0.0310\nOneHotRNN Epoch [80/100], Avg Loss: 0.0263\nOneHotRNN Epoch [81/100], Avg Loss: 0.0335\nOneHotRNN Epoch [82/100], Avg Loss: 0.0346\nOneHotRNN Epoch [83/100], Avg Loss: 0.0284\nOneHotRNN Epoch [84/100], Avg Loss: 0.0319\nOneHotRNN Epoch [85/100], Avg Loss: 0.0266\nOneHotRNN Epoch [86/100], Avg Loss: 0.0269\nOneHotRNN Epoch [87/100], Avg Loss: 0.0256\nOneHotRNN Epoch [88/100], Avg Loss: 0.0222\nOneHotRNN Epoch [89/100], Avg Loss: 0.0269\nOneHotRNN Epoch [90/100], Avg Loss: 0.0215\nOneHotRNN Epoch [91/100], Avg Loss: 0.0291\nOneHotRNN Epoch [92/100], Avg Loss: 0.0279\nOneHotRNN Epoch [93/100], Avg Loss: 0.0246\nOneHotRNN Epoch [94/100], Avg Loss: 0.0168\nOneHotRNN Epoch [95/100], Avg Loss: 0.0181\nOneHotRNN Epoch [96/100], Avg Loss: 0.0225\nOneHotRNN Epoch [97/100], Avg Loss: 0.0290\nOneHotRNN Epoch [98/100], Avg Loss: 0.0208\nOneHotRNN Epoch [99/100], Avg Loss: 0.0145\nOneHotRNN Epoch [100/100], Avg Loss: 0.0199\nOneHotRNN Training Time: 1659.50s\n\nEmbeddingLSTM Epoch [1/100], Avg Loss: 7.5628\nEmbeddingLSTM Epoch [2/100], Avg Loss: 6.5509\nEmbeddingLSTM Epoch [3/100], Avg Loss: 5.9550\nEmbeddingLSTM Epoch [4/100], Avg Loss: 5.4912\nEmbeddingLSTM Epoch [5/100], Avg Loss: 4.9932\nEmbeddingLSTM Epoch [6/100], Avg Loss: 4.4697\nEmbeddingLSTM Epoch [7/100], Avg Loss: 4.0693\nEmbeddingLSTM Epoch [8/100], Avg Loss: 3.7162\nEmbeddingLSTM Epoch [9/100], Avg Loss: 3.3194\nEmbeddingLSTM Epoch [10/100], Avg Loss: 2.9173\nEmbeddingLSTM Epoch [11/100], Avg Loss: 2.5887\nEmbeddingLSTM Epoch [12/100], Avg Loss: 2.2956\nEmbeddingLSTM Epoch [13/100], Avg Loss: 1.9633\nEmbeddingLSTM Epoch [14/100], Avg Loss: 1.6148\nEmbeddingLSTM Epoch [15/100], Avg Loss: 1.3254\nEmbeddingLSTM Epoch [16/100], Avg Loss: 1.0831\nEmbeddingLSTM Epoch [17/100], Avg Loss: 0.8929\nEmbeddingLSTM Epoch [18/100], Avg Loss: 0.7391\nEmbeddingLSTM Epoch [19/100], Avg Loss: 0.6107\nEmbeddingLSTM Epoch [20/100], Avg Loss: 0.5063\nEmbeddingLSTM Epoch [21/100], Avg Loss: 0.4236\nEmbeddingLSTM Epoch [22/100], Avg Loss: 0.3560\nEmbeddingLSTM Epoch [23/100], Avg Loss: 0.3026\nEmbeddingLSTM Epoch [24/100], Avg Loss: 0.2552\nEmbeddingLSTM Epoch [25/100], Avg Loss: 0.2274\nEmbeddingLSTM Epoch [26/100], Avg Loss: 0.2076\nEmbeddingLSTM Epoch [27/100], Avg Loss: 0.2038\nEmbeddingLSTM Epoch [28/100], Avg Loss: 0.2155\nEmbeddingLSTM Epoch [29/100], Avg Loss: 0.2400\nEmbeddingLSTM Epoch [30/100], Avg Loss: 0.3228\nEmbeddingLSTM Epoch [31/100], Avg Loss: 0.4452\nEmbeddingLSTM Epoch [32/100], Avg Loss: 0.4086\nEmbeddingLSTM Epoch [33/100], Avg Loss: 0.1796\nEmbeddingLSTM Epoch [34/100], Avg Loss: 0.0846\nEmbeddingLSTM Epoch [35/100], Avg Loss: 0.0478\nEmbeddingLSTM Epoch [36/100], Avg Loss: 0.0307\nEmbeddingLSTM Epoch [37/100], Avg Loss: 0.0210\nEmbeddingLSTM Epoch [38/100], Avg Loss: 0.0149\nEmbeddingLSTM Epoch [39/100], Avg Loss: 0.0111\nEmbeddingLSTM Epoch [40/100], Avg Loss: 0.0089\nEmbeddingLSTM Epoch [41/100], Avg Loss: 0.2216\nEmbeddingLSTM Epoch [42/100], Avg Loss: 0.1811\nEmbeddingLSTM Epoch [43/100], Avg Loss: 0.0496\nEmbeddingLSTM Epoch [44/100], Avg Loss: 0.0189\nEmbeddingLSTM Epoch [45/100], Avg Loss: 0.0099\nEmbeddingLSTM Epoch [46/100], Avg Loss: 0.0068\nEmbeddingLSTM Epoch [47/100], Avg Loss: 0.0050\nEmbeddingLSTM Epoch [48/100], Avg Loss: 0.0038\nEmbeddingLSTM Epoch [49/100], Avg Loss: 0.0029\nEmbeddingLSTM Epoch [50/100], Avg Loss: 0.0022\nEmbeddingLSTM Epoch [51/100], Avg Loss: 0.0016\nEmbeddingLSTM Epoch [52/100], Avg Loss: 0.0012\nEmbeddingLSTM Epoch [53/100], Avg Loss: 0.0009\nEmbeddingLSTM Epoch [54/100], Avg Loss: 0.0541\nEmbeddingLSTM Epoch [55/100], Avg Loss: 0.3108\nEmbeddingLSTM Epoch [56/100], Avg Loss: 0.0617\nEmbeddingLSTM Epoch [57/100], Avg Loss: 0.0155\nEmbeddingLSTM Epoch [58/100], Avg Loss: 0.0071\nEmbeddingLSTM Epoch [59/100], Avg Loss: 0.0046\nEmbeddingLSTM Epoch [60/100], Avg Loss: 0.0034\nEmbeddingLSTM Epoch [61/100], Avg Loss: 0.0026\nEmbeddingLSTM Epoch [62/100], Avg Loss: 0.0020\nEmbeddingLSTM Epoch [63/100], Avg Loss: 0.0015\nEmbeddingLSTM Epoch [64/100], Avg Loss: 0.0012\nEmbeddingLSTM Epoch [65/100], Avg Loss: 0.0009\nEmbeddingLSTM Epoch [66/100], Avg Loss: 0.0007\nEmbeddingLSTM Epoch [67/100], Avg Loss: 0.0005\nEmbeddingLSTM Epoch [68/100], Avg Loss: 0.0004\nEmbeddingLSTM Epoch [69/100], Avg Loss: 0.0003\nEmbeddingLSTM Epoch [70/100], Avg Loss: 0.0002\nEmbeddingLSTM Epoch [71/100], Avg Loss: 0.0002\nEmbeddingLSTM Epoch [72/100], Avg Loss: 0.2518\nEmbeddingLSTM Epoch [73/100], Avg Loss: 0.1173\nEmbeddingLSTM Epoch [74/100], Avg Loss: 0.0223\nEmbeddingLSTM Epoch [75/100], Avg Loss: 0.0066\nEmbeddingLSTM Epoch [76/100], Avg Loss: 0.0034\nEmbeddingLSTM Epoch [77/100], Avg Loss: 0.0024\nEmbeddingLSTM Epoch [78/100], Avg Loss: 0.0018\nEmbeddingLSTM Epoch [79/100], Avg Loss: 0.0014\nEmbeddingLSTM Epoch [80/100], Avg Loss: 0.0011\nEmbeddingLSTM Epoch [81/100], Avg Loss: 0.0008\nEmbeddingLSTM Epoch [82/100], Avg Loss: 0.0006\nEmbeddingLSTM Epoch [83/100], Avg Loss: 0.0005\nEmbeddingLSTM Epoch [84/100], Avg Loss: 0.0004\nEmbeddingLSTM Epoch [85/100], Avg Loss: 0.0003\nEmbeddingLSTM Epoch [86/100], Avg Loss: 0.0002\nEmbeddingLSTM Epoch [87/100], Avg Loss: 0.0002\nEmbeddingLSTM Epoch [88/100], Avg Loss: 0.0294\nEmbeddingLSTM Epoch [89/100], Avg Loss: 0.3015\nEmbeddingLSTM Epoch [90/100], Avg Loss: 0.0609\nEmbeddingLSTM Epoch [91/100], Avg Loss: 0.0122\nEmbeddingLSTM Epoch [92/100], Avg Loss: 0.0042\nEmbeddingLSTM Epoch [93/100], Avg Loss: 0.0026\nEmbeddingLSTM Epoch [94/100], Avg Loss: 0.0020\nEmbeddingLSTM Epoch [95/100], Avg Loss: 0.0015\nEmbeddingLSTM Epoch [96/100], Avg Loss: 0.0012\nEmbeddingLSTM Epoch [97/100], Avg Loss: 0.0009\nEmbeddingLSTM Epoch [98/100], Avg Loss: 0.0007\nEmbeddingLSTM Epoch [99/100], Avg Loss: 0.0005\nEmbeddingLSTM Epoch [100/100], Avg Loss: 0.0004\nEmbeddingLSTM Training Time: 950.20s\n\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"# Generate Poems\nseed_text = \"I wandered lonely as a\"\nprint(\"\\nGenerated Poem (OneHotRNN):\", generate_poem(onehot_model, seed_text, model_type = \"OneHotRNN\"))\nprint(\"\\nGenerated Poem (EmbeddingLSTM):\", generate_poem(embedding_model, seed_text, model_type = \"EmbeddingLSTM\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T11:39:16.302040Z","iopub.execute_input":"2025-03-24T11:39:16.302378Z","iopub.status.idle":"2025-03-24T11:39:16.469751Z","shell.execute_reply.started":"2025-03-24T11:39:16.302354Z","shell.execute_reply":"2025-03-24T11:39:16.469018Z"}},"outputs":[{"name":"stdout","text":"\nGenerated Poem (OneHotRNN): I wandered lonely as a woman in by boots to land, Look at the It is a melancholy show better hundred before, But I would send them. let me a melancholy said of the little steady will never laugh and ever so vapor to the brook heart which poured Annabel life or an reach A\n\nGenerated Poem (EmbeddingLSTM): I wandered lonely as a child that it with a love of love made there can shut while I know Mine they have you have I guess it is just as lucky to die, and I know it. I pass death with the dying and birth with the new-wash'd babe, and am not contain'd between\n","output_type":"stream"}],"execution_count":92}]}